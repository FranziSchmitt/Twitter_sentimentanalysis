{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# standard stuff\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import json\n",
    "import pickle \n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# SK-LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# custom imports\n",
    "from data_handling import load_data, collapse_dfs\n",
    "from text_preprocessing import df_processing, party_remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from pickle\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "collapse = False\n",
    "path_collapse = '/home/franzi/Git-DSR/Twitter_sentimentanalysis/Data/all_data_lda.pickle' # path where pickle file is saved\n",
    "\n",
    "if collapse:\n",
    "    all_data = collapse_dfs(aug_data)\n",
    "    df_processing(all_data)\n",
    "    all_data = all_data.drop_duplicates('clean_text')\n",
    "    all_data['user_id'] = all_data.loc[:, 'user'].map(lambda x: x['id_str'])\n",
    "    all_data.drop('user', inplace=True, axis=1)\n",
    "    print('dumping data to pickle')\n",
    "    with open(path_collapse, 'wb') as fid:\n",
    "        pickle.dump(all_data, fid)\n",
    "    print('data dumped')\n",
    "else: \n",
    "    print('loading data from pickle')\n",
    "    with open(path_collapse, 'rb') as fid:\n",
    "        all_data = pickle.load(fid)\n",
    "        print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stopwords_de,\n",
    "                    max_df=.1,\n",
    "                    max_features=5000)\n",
    "\n",
    "X = cv.fit_transform(all_data.loc[:, 'clean_text'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "                               random_state=123,\n",
    "                               learning_method='batch')\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "lda.components_.shape\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = cv.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print('Topic {}'.format(topic_idx + 1))\n",
    "    print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words -1:-1]]))\n",
    "\n",
    "def daily_topifier(df, cv_features, n_topics, n_top_words):\n",
    "    dates = ['2018-08-{:02d}'.format(i) for i in range(2,10)]\n",
    "    topics = {}\n",
    "    \n",
    "    for date in dates:\n",
    "        cv = CountVectorizer(stop_words=stopwords_de, max_df=.1, max_features=cv_features)\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=123, learning_method='batch')\n",
    "        X = cv.fit_transform(df.loc['{}'.format(date), 'clean_text'])\n",
    "        X_topics = lda.fit_transform(X)\n",
    "        feature_names = cv.get_feature_names()\n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            topics['{}, {}'.format(date, topic_idx + 1)] = [' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words -1:-1]])]\n",
    "            \n",
    "    return topics\n",
    "\n",
    "topics = daily_topifier(all_data, cv_features=5000, n_topics=5, n_top_words=5)\n",
    "\n",
    "topics\n",
    "# interesting: afd is very common but does not appear to be a topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
